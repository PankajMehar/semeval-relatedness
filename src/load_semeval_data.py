#!/usr/bin/python

"""
Data loading module for SemEval Shared Task 1.
"""

__author__ = 'Johannes Bjerva'
__email__  = 'j.bjerva@rug.nl'

import os
import shlex
import cPickle
import numpy as np
import requests
import xml.etree.ElementTree as et

from sys import stdout
from collections import defaultdict
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import WordPunctTokenizer
from nltk.tokenize.treebank import TreebankWordTokenizer
from subprocess import check_output, call

import drs_complexity
import config

# Tools for lemmatization/tokenization
wnl = WordNetLemmatizer()
wpt = WordPunctTokenizer()
twt = TreebankWordTokenizer()

# Used to encode the entailment judgements numerically
judgement_ids = defaultdict(lambda:len(judgement_ids))
prediction_ids = defaultdict(lambda:len(prediction_ids))
prover_ids = defaultdict(lambda:len(prover_ids))

def load_embeddings():
    """
    Load embeddings either from pre-processed binary, or fallback to txt if non-existant
    """
    num = config.vector_num
    if num == 1:
        vector_file = 'GoogleNews-vectors-negative300.txt'
    elif num == 2:
        vector_file = 'wiki_giga_vectors.txt'
    elif num == 3:
        vector_file = 'vectors_en.txt'

    try:
        if config.DEBUG: stdout.write('loading embedding {0} from archives.. '.format(num))

        with open('google_news_ids{0}.pickle'.format(num), 'rb') as in_f:
            word_ids = cPickle.load(in_f)     
        with open('google_news_np{0}.pickle'.format(num), 'rb') as in_f:
            projections = np.load(in_f)

    except IOError:
        if config.DEBUG: stdout.write(' error - processing txt-file instead (~15 mins)..')  

        projections, word_ids = load_word2vec(vector_file)
        with open('google_news_ids{0}.pickle'.format(num), 'wb') as out_f:
            cPickle.dump(dict(word_ids), out_f, -1)
        with open('google_news_np{0}.pickle'.format(num), 'wb') as out_f:
            np.save(out_f, projections)
    
    if config.DEBUG: stdout.write(' done!\n')

    return word_ids, projections

def load_word2vec(f_name):
    """
    Load word projections generated by word2vec from txt-rep.
    This takes a  while.
    """
    if config.DEBUG: stdout.write('\ngetting vocab size: ')
    vocab_size = int(check_output(shlex.split('wc -l {0}'.format(config.working_path+f_name))).split()[0])
    if config.DEBUG: stdout.write(str(vocab_size)+'\n')

    word_ids = defaultdict(lambda:len(word_ids))

    if config.DEBUG: print 'filling rep matrix'
    with open(config.working_path+f_name, 'r') as in_f:
        # First line is <s\>, i.e. unnecessary for our purposes
        # Use to get dimensionality and throw away.
        first = in_f.readline()
        dimensions = len(first.split()[1:])

        # NP matrix for word projections
        word_projections = np.zeros( (vocab_size, dimensions), dtype=np.float64)

        for line in in_f:
            fields = line.split()
            try:
                word_id = word_ids[fields[0].lower()]
                word_projections[word_id] = [float(i) for i in fields[1:]]
            except ValueError:
                # Error with embedding file - do nothing for now.
                pass

    return word_projections, word_ids


def load_sick_data():
    """
    Attempt to load sick data from binary,
    otherwise fall back to txt.
    """
    try:
        if config.DEBUG: stdout.write('loading sick from archives.. ')

        with open('sick.pickle', 'rb') as in_f:
            sick_data = cPickle.load(in_f)

    except IOError:
        if config.DEBUG: stdout.write(' error - loading from txt-files..')
        
        sick_data = []
        for line in open(os.path.join(config.working_path,'SICK_all.txt')):
            if line.split()[0] != 'pair_ID':
                sick_data.append(load_sick_data_from_folder(line.split()[0]))

        # Sort according to SICK_all.txt
        
        with open('sick.pickle', 'wb') as out_f:
            cPickle.dump(sick_data, out_f, -1)
    
    if config.DEBUG:
        stdout.write(' done!\n')

    return sick_data

def read_txt_file(path, delimeter):
    """
    Convert a txt file to a list using a delimeter
    """
    if os.path.isfile(path):
        return open(path).read().split(delimeter)
    else:
        # the file at path does not exist
        print 'None', path
        return None
    
def read_xml_file(path):
    """
    Read and parse an xml file
    """
    if os.path.isfile(path):
        return et.parse(path)
    else:
        # the file at path does not exist
        return None

def load_sick2_data_from_folder(id_folder):
    """
    Load the data from the sick2 folder
    """
    id_data = []
    id_data.append(id)                                                               #data[0]
    id_data.append(None)                                                             #data[1] gold.sim is not available/needed here
    id_data.append(read_txt_file(os.path.join(id_folder,'t'), ' '))                  #data[2]
    id_data.append(read_txt_file(os.path.join(id_folder,'h'), ' '))                  #data[3]
    id_data.append(read_txt_file(os.path.join(id_folder,'t.tok'), ' '))              #data[4]
    id_data.append(read_txt_file(os.path.join(id_folder,'h.tok'), ' '))              #data[5]
    id_data.append(read_txt_file(os.path.join(id_folder,'kt.mod'), '\n'))            #data[6]
    id_data.append(read_txt_file(os.path.join(id_folder,'kh.mod'), '\n'))            #data[7]
    id_data.append(read_txt_file(os.path.join(id_folder,'kth.mod'), '\n'))           #data[8]
    id_data.append(read_xml_file(os.path.join(id_folder,'t.xml')))                   #data[9]
    id_data.append(read_xml_file(os.path.join(id_folder,'h.xml')))                   #data[10]
    id_data.append(read_txt_file(os.path.join(id_folder,'modsizedif.txt'), '\n'))    #data[11]
    id_data.append(read_txt_file(os.path.join(id_folder,'prediction.txt'), '\n'))    #data[12]
    id_data.append(get_lemmas(id_data[2]))
    id_data.append(get_lemmas(id_data[3]))
    id_data.append([])                                                               #data[13] these are already replacements
    
    return id_data

def get_sick2_data(id):
    """
    Look for all the alternative sick folder (using paraphrases) and get the candc data from these folders
    """
    sick2_data = []
    for folder in os.listdir(config.shared_sick2):
        if str(folder).startswith("{0}.".format(id)):
            sick2_data.append(load_sick2_data_from_folder(os.path.join(config.shared_sick2, folder)))
    return sick2_data

def get_lemmas(sentence):
    return [wnl.lemmatize(word.lower().strip()) for word in sentence]

def load_sick_data_from_folder(id):
    """
    Load the data from the sick folder
    """
    id_folder = os.path.join(config.shared_sick,str(id))
    id_data = []
    id_data.append(id)                                                               #data[0]
    id_data.append(read_txt_file(os.path.join(id_folder,'gold.sim'), '\n')[0])       #data[1]
    id_data.append(read_txt_file(os.path.join(id_folder,'t'), ' '))                  #data[2]
    id_data.append(read_txt_file(os.path.join(id_folder,'h'), ' '))                  #data[3]
    id_data.append(read_txt_file(os.path.join(id_folder,'t.tok'), ' '))              #data[4]
    id_data.append(read_txt_file(os.path.join(id_folder,'h.tok'), ' '))              #data[5]
    id_data.append(read_txt_file(os.path.join(id_folder,'kt.mod'), '\n'))            #data[6]
    id_data.append(read_txt_file(os.path.join(id_folder,'kh.mod'), '\n'))            #data[7]
    id_data.append(read_txt_file(os.path.join(id_folder,'kth.mod'), '\n'))           #data[8]
    id_data.append(read_xml_file(os.path.join(id_folder,'t.xml')))                   #data[9]
    id_data.append(read_xml_file(os.path.join(id_folder,'h.xml')))                   #data[10]
    id_data.append(read_txt_file(os.path.join(id_folder,'modsizedif.txt'), '\n'))    #data[11]
    id_data.append(read_txt_file(os.path.join(id_folder,'prediction.txt'), '\n'))    #data[12]
    id_data.append(get_lemmas(id_data[2]))
    id_data.append(get_lemmas(id_data[3]))
    id_data.append(get_sick2_data(id))                                               #data[13]

    return id_data


url = 'http://127.0.0.1:7777/raw/pipeline?format=xml'
def get_and_write_complexities(pair_id, sentence_a, sentence_b):
    root = './working/sick/'+pair_id

    r = requests.post(url, data=' '.join(sentence_a))
    complexity_a = drs_complexity.parse_xml(r.text)

    r = requests.post(url, data=' '.join(sentence_b))
    complexity_b = drs_complexity.parse_xml(r.text)
    
    with open(root+'/complexities.txt', 'w') as out_f:
        out_f.write('{0}\n{1}\n'.format(complexity_a, complexity_b))

    print pair_id, complexity_a, complexity_b
    return (complexity_a, complexity_b)