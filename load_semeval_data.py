#!/usr/bin/python

"""
Data loading module for SemEval Shared Task 1.
"""

__author__ = 'Johannes Bjerva'
__email__  = 'j.bjerva@rug.nl'

import cPickle
import numpy as np
from sys import stdout
from collections import defaultdict
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import WordPunctTokenizer
from nltk.tokenize.treebank import TreebankWordTokenizer
from subprocess import check_output, call
import shlex

wvec_path = 'working/'

# Params
DEBUG = True

# Tools for lemmatization/tokenizationd /etc/fonts/conf.d/50-user.con
wnl = WordNetLemmatizer()
wpt = WordPunctTokenizer()
twt = TreebankWordTokenizer()

# Used to encode the entailment judgements numerically
judgement_ids = defaultdict(lambda:len(judgement_ids))

def load_embeddings():
    """
    Load embeddings either from pre-processed binary, or fallback to txt if non-existant
    """
    vector_file = 'working/GoogleNews-vectors-negative300.txt'#argv[1]

    try:
        if DEBUG: stdout.write('loading embeddings from archives.. ')

        with open('working/google_news_ids.pickle', 'rb') as in_f:
            word_ids = cPickle.load(in_f)     
        with open('working/google_news_np.pickle', 'rb') as in_f:
            projections = np.load(in_f)

    except IOError:
        if DEBUG: stdout.write(' error - processing txt-file instead (~15 mins)..')

        word_ids = defaultdict(lambda:len(word_ids))
        projections = load_word2vec(vector_file, word_ids)
        with open('working/google_news_ids.pickle', 'wb') as out_f:
            cPickle.dump(dict(word_ids), out_f, -1)
        with open('working/google_news_np.pickle', 'wb') as out_f:
            np.save(out_f, projections)
    
    if DEBUG: stdout.write(' done!\n')

    return word_ids, projections

def load_word2vec(f_name, word_ids):
    """
    Load word projections generated by word2vec from txt-rep.
    This takes a  while.
    """
    if DEBUG: stdout.write('getting vocab size: ')
    dimensions = 300 #int(f_name.lstrip('word_projections-')[:-4])
    vocab_size = int(check_output(shlex.split('wc -l {0}'.format(wvec_path+f_name))).split()[0])
    if DEBUG: stdout.write(str(vocab_size)+'\n')

    # NP matrix for word projections
    word_projections = np.zeros( (vocab_size, dimensions), dtype=np.float64)

    if DEBUG: print 'filling rep matrix'
    with open(wvec_path+f_name, 'r') as in_f:
        for line in in_f:
            fields = line.split()
            word_id = word_ids[fields[0].lower()]
            word_projections[word_id] = [float(i) for i in fields[1:]]

    return word_projections

def load_sick_data(f_name):
    """
    Load data from the sick data set.
    Lemmatize all words using NLTK
    """
    data = []
    with open(f_name, 'r') as in_f:
        header = in_f.readline()
        for line in in_f:
            fields = line.split('\t')
            pair_id = int(fields[0].strip())

            # Tokenizing gives a better Spearman correlation, but worse Pearson...
            #sentence_a = [wnl.lemmatize(word) for word in twt.tokenize(fields[1].strip().lower())]
            #sentence_b = [wnl.lemmatize(word) for word in twt.tokenize(fields[2].strip().lower())]

            sentence_a = [wnl.lemmatize(word) for word in fields[1].strip().lower().split()]
            sentence_b = [wnl.lemmatize(word) for word in fields[2].strip().lower().split()]
            
            relatedness = float(fields[3].strip())
            judgement = fields[4].strip()

            data.append((pair_id, sentence_a, sentence_b, relatedness, judgement_ids[judgement]))

    return data

def load_shared_sick_data(path):
    """
    Load shared sick data, parsed with boxer etc.
    TODO: Extract relevant features
    """
    data = []
    prefix = len(path)
    err, corr = 0,0
    for root, dirs, files in os.walk(path):
        if len(files) > 5:
            try:
                sick_id = int(root[prefix:])
                with open(root+'/t.tok', 'r') as in_f:
                    sentence_a = [wnl.lemmatize(word) for word in in_f.readline().lower().split()]

                with open(root+'/h.tok', 'r') as in_f:
                    sentence_b = [wnl.lemmatize(word) for word in in_f.readline().lower().split()]

                with open(root+'/gold.sim', 'r') as in_f:
                    relatedness = float(in_f.readline().strip())

                with open(root+'/gold.rte', 'r') as in_f:
                    judgement = in_f.readline().strip()


                data.append((sick_id, sentence_a, sentence_b, relatedness, judgement_ids[judgement]))
                corr += 1
            except IOError:
                err += 1

    print "done", corr
    print "fail", err
            
    #root = /home/p269101/Dev/candc2/candc/working/sick/24
    #files=
    #['t', 'h', 'gold.sim', 'gold.rte', 't.tok', 'h.tok', 'th.tok', 't.ccg', 'h.ccg', 
    #'th.ccg', 'prediction.txt', 'modsizedif.txt', 't.drs', 'h.drs', 'th.drs', 'kt.mwn', 
    #'kh.mwn', 'kth.mwn', 'vampire.in', 'paradox.in', 'vampire.out', 'paradox.out', 
    #'tpmb.out', 't.mod', 'h.mod', 'th.mod', 'tth.mod']

    return data
